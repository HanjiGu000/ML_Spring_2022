{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$$\n",
       "\\newcommand{\\x}{\\mathbf{x}}\n",
       "\\newcommand{\\tx}{\\tilde{\\x}}\n",
       "\\newcommand{\\y}{\\mathbf{y}}\n",
       "\\newcommand{\\b}{\\mathbf{b}}\n",
       "\\newcommand{\\c}{\\mathbf{c}}\n",
       "\\newcommand{\\e}{\\mathbf{e}}\n",
       "\\newcommand{\\z}{\\mathbf{z}}\n",
       "\\newcommand{\\h}{\\mathbf{h}}\n",
       "\\newcommand{\\u}{\\mathbf{u}}\n",
       "\\newcommand{\\v}{\\mathbf{v}}\n",
       "\\newcommand{\\w}{\\mathbf{w}}\n",
       "\\newcommand{\\W}{\\mathbf{W}}\n",
       "\\newcommand{\\X}{\\mathbf{X}}\n",
       "\\newcommand{\\KL}{\\mathbf{KL}}\n",
       "\\newcommand{\\E}{{\\mathbb{E}}}\n",
       "\\newcommand{\\ip}{\\mathbf{{(i)}}}\n",
       "%\n",
       "% Test set\n",
       "\\newcommand{\\xt}{\\underline{\\x}}\n",
       "\\newcommand{\\yt}{\\underline{\\y}}\n",
       "\\newcommand{\\Xt}{\\underline{\\X}}\n",
       "\\newcommand{\\perfm}{\\mathcal{P}}\n",
       "%\n",
       "% \\ll indexes a layer; we can change the actual letter\n",
       "\\newcommand{\\ll}{l}\n",
       "\\newcommand{\\llp}{{(\\ll)}}\n",
       "%\n",
       "\\newcommand{Thetam}{\\Theta_{-0}}\n",
       "\n",
       "% CNN\n",
       "\\newcommand{\\kernel}{\\mathbf{k}} \n",
       "\\newcommand{\\dim}{d}\n",
       "\\newcommand{\\idxspatial}{{\\text{idx}}}\n",
       "\\newcommand{\\summaxact}{\\text{max}}\n",
       "%\n",
       "%\n",
       "\n",
       "% RNN\n",
       "% \\tt indexes a time step\n",
       "\\newcommand{\\tt}{t}\n",
       "\\newcommand{\\tp}{{(\\tt)}}\n",
       "%\n",
       "%\n",
       "\n",
       "% LSTM\n",
       "\\newcommand{\\g}{\\mathbf{g}}\n",
       "\\newcommand{\\remember}{\\mathbf{remember}}\n",
       "\\newcommand{\\save}{\\mathbf{save}}\n",
       "\\newcommand{\\focus}{\\mathbf{focus}}\n",
       "%\n",
       "%\n",
       "% NLP\n",
       "\\newcommand{\\Vocab}{\\mathbf{V}}\n",
       "\\newcommand{\\v}{\\mathbf{v}}\n",
       "\\newcommand{\\offset}{o}\n",
       "\\newcommand{\\o}{o}\n",
       "\\newcommand{\\E}{\\mathbf{E}}\n",
       "%\n",
       "%\n",
       "\\newcommand{\\loss}{\\mathcal{L}}\n",
       "\\newcommand{\\cost}{\\mathcal{L}}\n",
       "%\n",
       "%                     \n",
       "\\newcommand{\\pdata}{p_\\text{data}}\n",
       "\\newcommand{\\pmodel}{p_\\text{model}}\n",
       "%\n",
       "% SVM\n",
       "\\newcommand{\\margin}{{\\mathbb{m}}}\n",
       "\\newcommand{\\lmk}{\\boldsymbol{\\ell}}\n",
       "%\n",
       "% Functions with arguments\n",
       "\\def\\xsy#1#2{#1^#2}\n",
       "\\def\\rand#1{\\tilde{#1}}\n",
       "\\def\\randx{\\rand{\\x}}\n",
       "\\def\\randy{\\rand{\\y}}\n",
       "\\def\\trans#1{\\dot{#1}}\n",
       "\\def\\transx{\\trans{\\x}}\n",
       "\\def\\transy{\\trans{\\y}}\n",
       "%\n",
       "\\def\\argmax#1{\\underset{#1} {\\operatorname{argmax}} }\n",
       "\\def\\argmin#1{\\underset{#1} {\\operatorname{argmin}} }\n",
       "\\def\\max#1{\\underset{#1} {\\operatorname{max}} }\n",
       "\\def\\min#1{\\underset{#1} {\\operatorname{min}} }\n",
       "%\n",
       "\\def\\pr#1{\\mathcal{p}(#1)}\n",
       "\\def\\prc#1#2{\\mathcal{p}(#1 \\; | \\; #2)}\n",
       "\\def\\cnt#1{\\mathcal{count}_{#1}}\n",
       "\\def\\node#1{\\mathbb{#1}}\n",
       "%\n",
       "\\newcommand{\\floor}[1]{\\left\\lfloor #1 \\right\\rfloor}\n",
       "\\newcommand{\\ceil}[1]{\\left\\lceil #1 \\right\\rceil}\n",
       "%\n",
       "\\def\\loc#1{{\\text{##} {#1}}}\n",
       "%\n",
       "$$\n"
      ],
      "text/plain": [
       "<IPython.core.display.Latex object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%run Latex_macros.ipynb\n",
    "%run beautify_plots.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "$$\n",
    "\\newcommand{\\R}{\\mathbf{R}}\n",
    "\\newcommand{\\r}{\\mathbf{r}}\n",
    "\\newcommand{\\F}{\\mathbf{F}}\n",
    "\\newcommand{\\V}{\\mathbf{V}}\n",
    "\\newcommand{\\ntickers}{{n_\\text{tickers}}}\n",
    "\\newcommand{\\ndates}{{n_\\text{dates}}}\n",
    "\\newcommand{\\nfactors}{{n_\\text{factors}}}\n",
    "\\newcommand{\\nchars}{{n_\\text{chars}}}\n",
    "\\newcommand{\\dp}{{(d)}}\n",
    "\\newcommand{\\sp}{{(s)}}\n",
    "\\newcommand{\\Bbeta}{\\mathbf\\beta}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Factor models via Autoencoders\n",
    "\n",
    "A clever way of using Neural Networks to solve a familiar but important problem in Finance\n",
    "was proposed by [Gu, Kelly, and Xiu, 2019](https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3335536).\n",
    "\n",
    "It is an extension of the Factor Model framework of Finance, combined with the tools of\n",
    "dimensionality reduction (to find the factors) of Deep Learning: the Autoencoder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "You can find [code](https://github.com/stefan-jansen/machine-learning-for-trading/blob/main/20_autoencoders_for_conditional_risk_factors/06_conditional_autoencoder_for_asset_pricing_model.ipynb)\n",
    "for this model as part of the excellent book by [Stefan Jansen](https://github.com/stefan-jansen/machine-learning-for-trading/blob/main/20_autoencoders_for_conditional_risk_factors/06_conditional_autoencoder_for_asset_pricing_model.ipynb)\n",
    "- [Github](https://github.com/stefan-jansen/machine-learning-for-trading)\n",
    "- In order to run the code notebook, you first need to run a notebook for [data preparation](https://github.com/stefan-jansen/machine-learning-for-trading/blob/main/20_autoencoders_for_conditional_risk_factors/05_conditional_autoencoder_for_asset_pricing_data.ipynb)\n",
    "    - This notebook relies on files created by notebooks from earlier chapters of the book\n",
    "    - So, if you want to run the code, you have a lot of preparatory work ahead of you\n",
    "    - Try to take away the ideas and the coding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Factor Model review\n",
    "\n",
    "We will begin with a quick review/introduction to Factor Models in Finance.\n",
    "\n",
    "\n",
    "First, some necessary notation:\n",
    "- $\\r^{(d)}_s$: Return of ticker $s$ on day $d$.\n",
    "- $\\hat\\r^{(d)}_s$: approximation of $\\r^{(d)}_s$\n",
    "\n",
    "- $n_\\text{tickers}$: **large** number of tickers\n",
    "- $n_\\text{dates}$ number of dates\n",
    "- $n_\\text{factors}$: **small** number of factors: independent variables (features) in our approximation\n",
    "- Returns matrix $\\R$ indexed by *date*\n",
    "    - $\\R: (n_\\text{dates} \\times n_\\text{tickers})$\n",
    "    - $|| \\R^\\dp || = n_\\text{tickers}$\n",
    "        - $\\R^\\dp$ is vector of returns for each of the $\\ntickers$ on date $d$\n",
    "\n",
    "- $\\r$ will denote a vector of single day returns: $\\R^\\dp$ for some date $d$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Notation summary**\n",
    "\n",
    "term &nbsp; &nbsp;  &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;| meaning\n",
    ":---|:---\n",
    "$s$ | ticker\n",
    "$\\ntickers$ | number of tickers\n",
    "$d$ | date\n",
    "$\\ndates$ | number of dates\n",
    "$\\nchars$   | number of characteristics per ticker\n",
    "$m$ | number of examples\n",
    "    | $m = \\ndates$\n",
    "$i$ | index of example\n",
    "    | There will be one example per date, so we use $i$ and $d$ interchangeably.\n",
    "$ [ \\X^\\ip, \\R^\\ip ]$ | example $i$\n",
    "         | $|| \\X^\\ip || = (\\ntickers \\times  \\nchars )$ \n",
    "         | $|| \\R^\\ip || = \\ntickers$\n",
    "$\\X^\\dp_s$ | vector of ticker $s$'s characteristics on day $d$\n",
    "             | $ || \\X^\\dp_s || = \\nchars$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Note**\n",
    "\n",
    "The paper actually seeks to predict $\\hat\\r^{(d+1)}_s$ (forward return) rather than approximate\n",
    "the current return $\\hat\\r^\\dp_s$.\n",
    "\n",
    "We will present this as an approximation problem as opposed to a prediction problem for\n",
    "simplicity of presentation (i.e., to include PCA as a model)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "A **factor model** seeks to approximate/explain the return of a *number* of tickers in terms of common \"factors\" $\\F$\n",
    "- $\\F: (\\ndates \\times \\nfactors)$\n",
    "$$\n",
    "\\begin{array} \\\\\n",
    "\\R^\\dp_1 & = & \\Bbeta^\\dp_1 \\cdot \\F^\\dp  + \\epsilon_1\\\\\n",
    "\\vdots \\\\\n",
    "\\R^\\dp_\\ntickers & = & \\Bbeta^\\dp_\\ntickers \\cdot \\F^\\dp + \\epsilon_\\ntickers \\\\\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "\n",
    "There are several ways to create a factor model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Pre-defined factors, solve for sensitivities\n",
    "\n",
    "First: supposed $\\F$ is given\n",
    "- For each date $d$, returns for: market, several industries, large/small cap\n",
    "- Solve for $\\Bbeta_s$, for each $s$\n",
    "    - $\\ntickers$ separate Linear Regression models: $\\langle \\X^\\dp, \\y^\\dp \\rangle = \\langle \\r^\\dp_s, \\F^\\dp \\rangle$\n",
    "    - Regression of time-series of a ticker's return agains a time-series of Factor returns\n",
    "    - Solve for $\\Bbeta_s$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Pre-defined sensitivities, solve for factors\n",
    "\n",
    "Alternately: suppose $\\Bbeta$ is given\n",
    "- For each ticker $s$, sensitivity of $s$ to $\\Bbeta_j$\n",
    "- Solve for $\\F^\\dp$, for each $d$\n",
    "    - $\\ndates$ separate Linear Regression models $\\langle \\X^\\sp, \\y^\\sp \\rangle = \\langle \\Bbeta_s, \\r^\\dp_s \\rangle$\n",
    "    - Regression of *cross-section* of tickers returns against a cross-section of ticker sensitivities\n",
    "    - Solve for $\\F^\\dp$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Solve for sensitivities and factors: PCA\n",
    "\n",
    "Yet another possibility: solve for $\\Bbeta$ and $\\F$ *simulataneoulsy*.\n",
    "\n",
    "Recall Principal Components\n",
    "- Representing $\\X$ (with \"standard\" basis vectors) via an *alternate basis* $\\V$\n",
    "$$\\X = \\tilde\\X \\V^T$$\n",
    "\n",
    "In this case without dimensionality reduction:\n",
    "$$\n",
    "\\R = \\tilde\\R V^T\n",
    "$$\n",
    "\n",
    "where\n",
    "$$\n",
    "\\begin{array} \\\\\n",
    "\\R, \\tilde\\R: (\\ndates \\times n_\\text{tickers}) \\\\\n",
    "\\V^T: (n_\\text{tickers} \\times n_\\text{tickers} ) \\\\\n",
    "\\end{array}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "With dimensionality reduced from $n_\\text{tickers}$ to $n_\\text{factors}$\n",
    "\n",
    "$\\R = \\F \\, \\Bbeta^T$\n",
    "\n",
    "- $\\F^T: (\\ndates \\times \\nfactors)$\n",
    "    - is $\\tilde\\R$ with columns eliminated b/c of dimensionality reduction\n",
    "- $\\Bbeta^T: (  \\nfactors \\times \\ntickers)$  \n",
    "    - so $\\Bbeta^\\sp$ are sensitivities of $s$ to factors\n",
    "\n",
    "- Solve for $\\F, \\Bbeta$ simultaneously\n",
    "\n",
    "The daily observation of $\\ntickers$ returns $\\R^\\dp$ is replaced by $\\nfactors$ returns $\\F^\\dp$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# This paper\n",
    "\n",
    "This paper will create a factor model that\n",
    "- Solve for $\\F, \\Bbeta$ simultaneously\n",
    "    - like PCA\n",
    "- **But** where $\\F$ and $\\Bbeta$ are defined by Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Autoencoder\n",
    "\n",
    "The paper refers to the model as a kind of Autoencoder.\n",
    "\n",
    "Let's review the topic.\n",
    "    \n",
    "Training examples $\\langle \\X^\\dp, \\y^\\dp \\rangle = \\langle \\R^\\dp, \\R^\\dp \\rangle$\n",
    "\n",
    "No obvious form as factor model\n",
    "- $\\R^\\dp = \\r$ \n",
    "    - mapped by Encoder to latent $\\z$ (of length $\\nfactors$)\n",
    "    - latent $\\z$ mapped to $\\r$ by Decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Imagine instead creating an \"Autoencoder\" that worked as follows\n",
    "- Maps $\\R^\\dp$ to $\\Bbeta^\\dp$\n",
    "    - sensitivity of each of the $\\ntickers$ on day $d$ to  day $d$ returns of $\\nfactors$ $\\F^\\dp$\n",
    "-  Maps $\\R^\\dp$ to the day $d$ returns of $\\nfactors$ $\\F^\\dp$\n",
    "- Outputting $\\y^\\dp = \\Bbeta^\\dp \\, \\F^\\dp$\n",
    "\n",
    "It acts as an Autoencoder in the senses that the Training examples $\\langle \\X^\\dp, \\y^\\dp \\rangle = \\langle \\R^\\dp, \\R^\\dp \\rangle$\n",
    "- But constrains $\\hat\\y^\\dp = \\hat\\R^\\dp$ to the form $\\hat\\R^\\dp = \\Bbeta^\\dp \\, \\F^\\dp$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "This model solves for $\\Bbeta^\\dp, \\F^\\dp$ simultaneously\n",
    "- almost what  PCA does **but**, in PCA,  $\\Bbeta$ does not vary by day\n",
    "- this model: the beta of a ticker $s$ to a factor $j$ changes by day $d$ !\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "This paper goes one step further than the standard Autoencoder\n",
    "- Standard Autoencoder maps $\\R^\\dp$ to $\\Bbeta^\\dp$\n",
    "- This paper allows $\\nchars \\ge 1$ daily *characteristics* $\\X^\\dp$ to map to $\\Bbeta^\\dp$\n",
    "    - one characteristic may be $\\R^\\dp$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "$\\beta^\\dp_s =  \\text{NN}( \\X^\\dp_s ; \\W_\\Bbeta )$\n",
    "- $\\beta^\\dp_s$ \n",
    "    - parameterized by weights $\\W_\\Bbeta$\n",
    "    - is only a function of the characteristics of $s$\n",
    "    - **not** a function of *other* ticker $s'$ characteristics $\\X_{s'}$, as in PCA\n",
    "- $\\beta^\\dp_{s}$ share the same weights $\\W_\\beta$ for all $s, d$\n",
    "    - unlike fixed factor, solve for $\\beta_s$\n",
    "        - different for each $s$\n",
    "        - same for each day $d$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### This model: nothing pre-defined, solve for sensitivities and factors\n",
    "- Simultaneously solve for $\\beta^\\dp_s$ and $\\F^\\dp$\n",
    "    - $\\Bbeta^\\dp_s$ constrained: \n",
    "    $$\n",
    "\\beta^\\dp_s = \\text{Dense} \\,( \\nfactors ) ( \\X^\\dp_s )\n",
    "$$\n",
    "        - combination of ticker-specific, time-varying characteristics $\\X^\\dp_s$\n",
    "        - we solve for the *combining weights*\n",
    "            - shared by all tickers and dates\n",
    "    - $\\F^\\dp$ constrained\n",
    "    $$\n",
    "\\F^\\dp = \\text{Dense} \\,( \\nfactors ) ( \\R^\\dp )\n",
    "$$\n",
    "\n",
    "        - combination of time-varying *raw returns* $\\R^\\dp$ \n",
    "        - we solve for *combining weights*\n",
    "            - shared by all dates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<table>\n",
    "    <tr>\n",
    "        <th><center>Autoencoder for Conditional Risk Factors</center></th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td><img src=\"images/Autoencoder_for_conditional_risk_factors.png\" width=\"90%\"></td>\n",
    "    </tr>\n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## This paper\n",
    "- $\\r^\\dp = \\Bbeta^\\dp * \\r^\\dp$\n",
    "    - $\\r^\\dp$ shape is $(n_\\text{tickers} \\times 1)$\n",
    "    - $\\Bbeta$ shape is $(n_\\text{tickers} \\times n_\\text{factors})$\n",
    "    - $\\r^\\dp$ shape is $(n_\\text{factors} \\times 1)$\n",
    "- Solve simultaneously for $\\Bbeta^\\dp, \\r^\\dp$    \n",
    "    where\n",
    "$\\beta^\\dp_s =  f( \\X^\\dp_s )$\n",
    "    - $\\beta^\\dp_s$ is only a function of the characteristics of $s$\n",
    "    - **not** $f( \\r^\\dp)$: the simultaneous returns of *other* $s'$ as in PCA\n",
    "    - $\\beta^\\dp_{s}$ share the same $\\W_\\beta$ for all $s, d$\n",
    "        - unlike fixed factor, solve for $\\beta_s$\n",
    "            - different for each $s$\n",
    "            - same for each day $d$\n",
    "\n",
    "    and where\n",
    "    \n",
    "    $\\r^\\dp = f(\\r^\\dp)$ for $f$ fixed over all $d$ \n",
    "        - like PCA\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Input side of network\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## Input $\\X$\n",
    "\n",
    "$\n",
    "\\X : ( \\ndates \\times \\ntickers \\times \\nchars )\n",
    "$\n",
    "\n",
    "$|| \\X^ || =  (\\ntickers \\times \\nchars)$\n",
    "- one example per date\n",
    "- example shape is $\\ntickers \\times \\nchars$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Dense $\\beta$\n",
    "\n",
    "- `Dense` $( \\nfactors )$\n",
    "    - `Dense`( $\\nfactors ) :  (\\ntickers \\times \\nchars) \\mapsto (\\ntickers \\times \\nfactors) $\n",
    "    - threads over ticker dimension ([see](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Dense))\n",
    "        - tickers share same weights\n",
    "        - single `Dense`( $\\nfactors )$ **not** $\\ntickers$ copies of `Dense`$( \\nfactors )$\n",
    "         \n",
    "- $\\W_\\beta: ( \\nfactors \\times \\nchars )$\n",
    "    - same across all $d, s$\n",
    "    - $W_\\beta^\\dp = W_\\Bbeta^{(d')}$ like any other training (same weight for every example)\n",
    "    - $W^\\dp_{\\Bbeta,(s)} = W^{(d)}_{\\Bbeta,(s')}$: transformation of characteristics to beta *independent* of ticker \n",
    "    - hence, size of $\\W_\\beta$ is $( \\nfactors \\times \\nchars )$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "$$\n",
    "\\beta^\\dp = \\text{Dense} \\,( \\nfactors ) ( \\X^\\dp )\n",
    "$$\n",
    "\n",
    "$$ || \\Bbeta^\\dp || = ( \\ntickers \\times \\nfactors )$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Factor side of network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Input $\\R$\n",
    "\n",
    "$\n",
    "\\R : ( \\ndates \\times \\ntickers )\n",
    "$\n",
    "\n",
    "$|| \\R^\\dp || =  (\\ntickers)$\n",
    "- one set of returns per date"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Dense $\\delta$ (factor)\n",
    "\n",
    "- `Dense` $( \\nfactors )$\n",
    "    - `Dense`( $\\nfactors ) :  \\ntickers \\mapsto \\nfactors$\n",
    "- $\\W_f: (\\nfactors \\times \\ntickers )$\n",
    "    - same across all $d, s$\n",
    "    - $W_f^\\dp = W_f^{(d')}$ like any other training (same weight for every example)\n",
    "    - $W^dp{f} $: transformation of ticker returns to factor returns \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "$$\n",
    "\\F^{(d)} = \\text{Dense} \\,( n_\\text{factors} ) ( \\R^{(d)} )\n",
    "$$\n",
    "\n",
    "$$ || \\F^{(d)} || =  n_\\text{factors}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Dot\n",
    "\n",
    "$$\\hat{\\r}^\\dp = \\Bbeta^\\dp \\cdot \\F^\\dp$$\n",
    "- Dot product threads over factor dimension\n",
    "\n",
    "$$ || \\hat{\\r}^\\dp || = \\ntickers $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Loss\n",
    "\n",
    "\n",
    "Let\n",
    "$\\loss^\\dp_{(s)}$ denote error of ticker $s$ on day $d$.\n",
    "$$\n",
    "\\loss^\\dp_{(s)} = \\r^\\dp_s - \\hat{\\r}^\\dp_s\n",
    "$$\n",
    "or perhaps \n",
    "$$\n",
    "\\loss^\\dp_\\sp = \\r^{(d+1)}_s - \\hat{\\r}^\\dp_s\n",
    "$$\n",
    "\n",
    "\n",
    "- $\\loss^\\dp$ is the loss of example $d$\n",
    "    - this loss has $\\ntickers$ sub-components\n",
    "    - This appears in example $i = d: \\X^\\dp$\n",
    "    - $\\loss^\\ip = \\loss^\\dp = \\sum_s { \\loss^\\dp_{(s)} }$\n",
    "- This is different than the loss $\\loss'$ for the case where an example is a single ticker on a single day\n",
    "    - $m' = n_\\text{dates} * \\ntickers$ examples in this case\n",
    "    - $\\loss'^\\ip = \\loss^\\dp_\\sp$"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "369.594px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
