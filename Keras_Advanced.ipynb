{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$$\n",
       "\\newcommand{\\x}{\\mathbf{x}}\n",
       "\\newcommand{\\tx}{\\tilde{\\x}}\n",
       "\\newcommand{\\y}{\\mathbf{y}}\n",
       "\\newcommand{\\b}{\\mathbf{b}}\n",
       "\\newcommand{\\c}{\\mathbf{c}}\n",
       "\\newcommand{\\e}{\\mathbf{e}}\n",
       "\\newcommand{\\z}{\\mathbf{z}}\n",
       "\\newcommand{\\h}{\\mathbf{h}}\n",
       "\\newcommand{\\u}{\\mathbf{u}}\n",
       "\\newcommand{\\v}{\\mathbf{v}}\n",
       "\\newcommand{\\w}{\\mathbf{w}}\n",
       "\\newcommand{\\W}{\\mathbf{W}}\n",
       "\\newcommand{\\X}{\\mathbf{X}}\n",
       "\\newcommand{\\KL}{\\mathbf{KL}}\n",
       "\\newcommand{\\E}{{\\mathbb{E}}}\n",
       "\\newcommand{\\ip}{\\mathbf{{(i)}}}\n",
       "%\n",
       "% Test set\n",
       "\\newcommand{\\xt}{\\underline{\\x}}\n",
       "\\newcommand{\\yt}{\\underline{\\y}}\n",
       "\\newcommand{\\Xt}{\\underline{\\X}}\n",
       "\\newcommand{\\perfm}{\\mathcal{P}}\n",
       "%\n",
       "% \\ll indexes a layer; we can change the actual letter\n",
       "\\newcommand{\\ll}{l}\n",
       "\\newcommand{\\llp}{{(\\ll)}}\n",
       "%\n",
       "\\newcommand{Thetam}{\\Theta_{-0}}\n",
       "\n",
       "% CNN\n",
       "\\newcommand{\\kernel}{\\mathbf{k}} \n",
       "\\newcommand{\\dim}{d}\n",
       "\\newcommand{\\idxspatial}{{\\text{idx}}}\n",
       "\\newcommand{\\summaxact}{\\text{max}}\n",
       "%\n",
       "%\n",
       "\n",
       "% RNN\n",
       "% \\tt indexes a time step\n",
       "\\newcommand{\\tt}{t}\n",
       "\\newcommand{\\tp}{{(\\tt)}}\n",
       "%\n",
       "%\n",
       "\n",
       "% LSTM\n",
       "\\newcommand{\\g}{\\mathbf{g}}\n",
       "\\newcommand{\\remember}{\\mathbf{remember}}\n",
       "\\newcommand{\\save}{\\mathbf{save}}\n",
       "\\newcommand{\\focus}{\\mathbf{focus}}\n",
       "%\n",
       "%\n",
       "% NLP\n",
       "\\newcommand{\\Vocab}{\\mathbf{V}}\n",
       "\\newcommand{\\v}{\\mathbf{v}}\n",
       "\\newcommand{\\offset}{o}\n",
       "\\newcommand{\\o}{o}\n",
       "\\newcommand{\\E}{\\mathbf{E}}\n",
       "%\n",
       "%\n",
       "\\newcommand{\\loss}{\\mathcal{L}}\n",
       "\\newcommand{\\cost}{\\mathcal{L}}\n",
       "%\n",
       "%                     \n",
       "\\newcommand{\\pdata}{p_\\text{data}}\n",
       "\\newcommand{\\pmodel}{p_\\text{model}}\n",
       "%\n",
       "% SVM\n",
       "\\newcommand{\\margin}{{\\mathbb{m}}}\n",
       "\\newcommand{\\lmk}{\\boldsymbol{\\ell}}\n",
       "%\n",
       "% Functions with arguments\n",
       "\\def\\xsy#1#2{#1^#2}\n",
       "\\def\\rand#1{\\tilde{#1}}\n",
       "\\def\\randx{\\rand{\\x}}\n",
       "\\def\\randy{\\rand{\\y}}\n",
       "\\def\\trans#1{\\dot{#1}}\n",
       "\\def\\transx{\\trans{\\x}}\n",
       "\\def\\transy{\\trans{\\y}}\n",
       "%\n",
       "\\def\\argmax#1{\\underset{#1} {\\operatorname{argmax}} }\n",
       "\\def\\argmin#1{\\underset{#1} {\\operatorname{argmin}} }\n",
       "\\def\\max#1{\\underset{#1} {\\operatorname{max}} }\n",
       "\\def\\min#1{\\underset{#1} {\\operatorname{min}} }\n",
       "%\n",
       "\\def\\pr#1{\\mathcal{p}(#1)}\n",
       "\\def\\prc#1#2{\\mathcal{p}(#1 \\; | \\; #2)}\n",
       "\\def\\cnt#1{\\mathcal{count}_{#1}}\n",
       "\\def\\node#1{\\mathbb{#1}}\n",
       "%\n",
       "\\newcommand{\\floor}[1]{\\left\\lfloor #1 \\right\\rfloor}\n",
       "\\newcommand{\\ceil}[1]{\\left\\lceil #1 \\right\\rceil}\n",
       "%\n",
       "\\def\\loc#1{{\\text{##} {#1}}}\n",
       "%\n",
       "$$\n"
      ],
      "text/plain": [
       "<IPython.core.display.Latex object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%run Latex_macros.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Advanced Keras: motivation\n",
    "\n",
    "We have been using the Keras Model API (mostly the `Sequential`) as a black box.\n",
    "\n",
    "But it is highly customizable\n",
    "- A `Model` is a class (as in Python object)\n",
    "- It implements methods such as\n",
    "    - `compile`\n",
    "    - `fit`\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We can change the behavior of a model in several ways\n",
    "- Arguments to some methods are objects; we can pass non-default functions/objects\n",
    "    - e.g., custom loss function\n",
    "- We can override these (and other) methods to make our models do new things."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The `Layer` is also an abstract class (Python) in Keras.\n",
    "\n",
    "Hence\n",
    "- We can create new layer types\n",
    "- We can override the methods of a given layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "In this module\n",
    "- we will\n",
    "illustrate techniques that you can use to customize your Layers/Models.\n",
    "- Illustrate the Functional model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Functional model: the basics, illustrated by the Transformer\n",
    "\n",
    "The `Sequential` model\n",
    "- organizes layers as an ordered list\n",
    "- restricts the input to layer $(\\ll+1)$ to be the output of layer $ll$.\n",
    "\n",
    "The `Functional` model\n",
    "- imposes **no** ordering on layers\n",
    "- imposes **no** restriction on connect outputs of one layer to the input of another"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "To illustrate the `Functional` model let's take a first look\n",
    "at model implementing a single `Transformer` block\n",
    "- we will revisit this code later to illustrate other concepts\n",
    "\n",
    "Here is the picture of a Transformer block\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <th><center>Transformer (Encoder/Decoder)</center></th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td><img src=\"images/Attention_is_all_u_need_Transformer.png\" width=50%></td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "There are actually 3 models in this [cell](https://colab.research.google.com/github/keras-team/keras-io/blob/master/examples/nlp/ipynb/neural_machine_translation_with_transformer.ipynb#scrollTo=cadZM4xkYDon) we will visit !\n",
    "\n",
    "Let's examine each one and try to relate the actual code to our picture.\n",
    "\n",
    "**pay close attention to the difference between $\\bar\\y$ (Encoder states) and $\\y$ (Decoder outputs)**\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <th><center>Transformer Layer (Encoder/Decoder)</center></th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td><img src=\"images/Transformer_Encoder_Decoder.png\" width=70%></td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## First model: the Encoder side of the Transformer\n",
    "\n",
    "The Encoder side of the transformer:\n",
    "\n",
    "    encoder_inputs = keras.Input(shape=(None,), dtype=\"int64\", name=\"encoder_inputs\")\n",
    "    x = PositionalEmbedding(sequence_length, vocab_size, embed_dim)(encoder_inputs)\n",
    "    encoder_outputs = TransformerEncoder(embed_dim, latent_dim, num_heads)(x)\n",
    "    encoder = keras.Model(encoder_inputs, encoder_outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "This illustrates the pattern common to `Functional` models\n",
    "- The output of a layer is assigned to a variable (e.g., `encoder_inputs` has the value of the model's inputs)\n",
    "- The output of a layer is connected to the input of another layer via \"function call\" syntax\n",
    "    - e.g., `encoder_inputs` is applied as the input to the `PositionalEmbedding` layer\n",
    "    \n",
    "        x = PositionalEmbedding(sequence_length, vocab_size, embed_dim)(encoder_inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The collection (not necessarily a sequence) of `Layer` calls defines a graph of function calls that maps\n",
    "`Model` inputs to outputs.\n",
    "\n",
    "To turn this collection into a `Model`\n",
    "- We define which function to feed Model inputs to\n",
    "- We define which function's outputs are the output of the model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "For example, we define the Encoder side (sub-model of the Transformer) of the Transformer via\n",
    "\n",
    "    encoder = keras.Model(encoder_inputs, encoder_outputs)\n",
    "    \n",
    "This defines `encoder` to be a `Model` with\n",
    "- input: `Layer` `encoder_inputs` (i.e., the `Input` layer)\n",
    "- output: `Layer` `encoder_outputs` (i.e., the `TransformerEncoder`)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Note**: the input and output of a `Model` *don't have to be* `Layer` types !\n",
    "\n",
    "There is also a model for the Decoder side of the Transformer in the cell we will visit:\n",
    "\n",
    "    decoder = keras.Model([decoder_inputs, encoded_seq_inputs], decoder_outputs)\n",
    "\n",
    "- input: An **array**  of 2 `Layer` types -- `[ decoder_inputs, encoded_seq_inputs ]`\n",
    "- output: `Layer`: `decoder_outputs`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- The output sequence $\\bar\\y_{(1..\\bar{T})}$ (i.e., latent states) of the Encoder\n",
    "    - Used in Decoder-Encoder attention\n",
    "    - $|| \\bar\\y || = \\bar{T} = \\text{length of Transformer input}$\n",
    "- The prefix of the Decoder outputs generated up to time $\\tt$\n",
    "    - The Decoder output at time $(\\tt-1)$ is appended to the Decoder inputs available at time $\\tt$\n",
    "    - So the inputs are the Decoder outputs $\\y_{(1..T)}$\n",
    "        - $T$ is *full* length of Transformer output\n",
    "        - Causal (Masked) Attention is used to restrict the Decoder\n",
    "            - from attending at step $\\tt$ to any $\\y_\\tp$ where $\\tt > (\\tt-1)$\n",
    "            - Can't look at an output that hasn't been generated yet !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Hence, the Decoder side takes a **pair** of inputs, as per the diagram.\n",
    "\n",
    "    \n",
    "    decoder = keras.Model([decoder_inputs, encoded_seq_inputs], decoder_outputs)\n",
    "\n",
    "Let's see if we can trace which role each element of the pair serves."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Let's examine the Encoder code more closely\n",
    "\n",
    "    encoder_inputs = keras.Input(shape=(None,), dtype=\"int64\", name=\"encoder_inputs\")\n",
    "    x = PositionalEmbedding(sequence_length, vocab_size, embed_dim)(encoder_inputs)\n",
    "    encoder_outputs = TransformerEncoder(embed_dim, latent_dim, num_heads)(x)\n",
    "    encoder = keras.Model(encoder_inputs, encoder_outputs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "First, observe that the *Encoder* outputs ($\\bar\\y_{(1..\\bar{T})}$) `encoder_outputs`\n",
    "\n",
    "    encoder = keras.Model(encoder_inputs, encoder_outputs)\n",
    "\n",
    "We see that these Encoder outputs become the second part of the pair of the actual arguments that are the inputs to the *Decoder*\n",
    "\n",
    "    decoder_outputs = decoder([decoder_inputs, encoder_outputs])\n",
    "    \n",
    "And the formal argument of `decoder` definition is `encoded_seq_inputs`\n",
    "\n",
    "    decoder = keras.Model([decoder_inputs, encoded_seq_inputs], decoder_outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "So the `encoder_outputs` ($\\bar\\y_{(1..\\bar{T})}$) become bound to `encoded_seq_inputs` within the Decoder.\n",
    "\n",
    "Hence, the second part of the input pair of `decoder` serves the role of $\\bar\\y_{(1..\\bar{T})}$, the sequence of Encoder latent states"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "\n",
    "## Second model: The Decoder side of the Transformer\n",
    "\n",
    "Now, let's look at the Decoder.\n",
    "\n",
    "    decoder_inputs = keras.Input(shape=(None,), dtype=\"int64\", name=\"decoder_inputs\")\n",
    "    encoded_seq_inputs = keras.Input(shape=(None, embed_dim), name=\"decoder_state_inputs\")\n",
    "    x = PositionalEmbedding(sequence_length, vocab_size, embed_dim)(decoder_inputs)\n",
    "    x = TransformerDecoder(embed_dim, latent_dim, num_heads)(x, encoded_seq_inputs)\n",
    "    x = layers.Dropout(0.5)(x)\n",
    "    decoder_outputs = layers.Dense(vocab_size, activation=\"softmax\")(x)\n",
    "    decoder = keras.Model([decoder_inputs, encoded_seq_inputs], decoder_outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "By tracing variable `x` backwards from the Decoder output ($\\y_{(1..T)}$) `decoder_outputs`\n",
    "\n",
    "    decoder_outputs = layers.Dense(vocab_size, activation=\"softmax\")(x)\n",
    "\n",
    "we can see the outputs derive from `Layer` sub-type `TransformerDecoder` \n",
    "\n",
    "    x = TransformerDecoder(embed_dim, latent_dim, num_heads)(x, encoded_seq_inputs)\n",
    "\n",
    "We have already shown that `encoded_seq_inputs` corresponds to $\\bar\\y_{(1..\\bar{T})}$\n",
    "\n",
    "So we can guess that variable `x` in the call to `TransformerDecoder` corresponds to $\\y_{(1..T)}$\n",
    "\n",
    "Let's confirm that."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Tracing variable `x` backwards from its use a the first argument in the `TransformerDecoder`\n",
    "\n",
    "    x = PositionalEmbedding(sequence_length, vocab_size, embed_dim)(decoder_inputs)\n",
    "    \n",
    "we see that it is the positionally-encoded (to enable Causal masking) `decoder_inputs` \n",
    "- The `PositionalEmbedding` is added to enforce Masking (causal ordering)\n",
    "\n",
    "Hopefully: `decoder_inputs` are the `decoder_outputs` shifted by one time step\n",
    "- That is: the training set enforces \"Teacher Forcing\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Third model: the full Transformer -- Encoder + Decoder\n",
    "\n",
    "Finally, there is the Transformer Model, combining an Encoder and Decoder:\n",
    "\n",
    "    decoder_outputs = decoder([decoder_inputs, encoder_outputs])\n",
    "    transformer = keras.Model(\n",
    "        [encoder_inputs, decoder_inputs], decoder_outputs, name=\"transformer\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The `transformer` input is a pair\n",
    "\n",
    "    [encoder_inputs, decoder_inputs]\n",
    "\n",
    "And it's output is\n",
    "\n",
    "     decoder_outputs\n",
    "\n",
    "We identify the first part of the input pair (`encoder_inputs`) as the input sequence $\\x_{(1\\dots \\bar{T})}$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Summary\n",
    "\n",
    "A lot going on here !\n",
    "- A complex connection of `Layer` outputs to inputs\n",
    "- Custom `Layer` sub-types\n",
    "    - `PositionalEmbedding`, `TransformerEncoder`, `TransformerDecoder`\n",
    "    - We will soon see how to define our own `Layer` sub-classes\n",
    "- Hopefully:\n",
    "    - `decoder_inputs` is equal to `decoder_outputs` shifted by one time step\n",
    "    - Teacher forcing, enforced by the organization of the training data ?\n",
    "\n",
    "That concludes are first look at the Functional model\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Model specialization\n",
    "\n",
    "## Custom loss (passing in a loss function)\n",
    "\n",
    "In introducing Deep Learning, we have asserted that\n",
    ">It's all about the Loss function\n",
    "\n",
    "That is: the key to solving many Deep Learning problems\n",
    "- Is not in devising a complex network architecture\n",
    "- But in writing a Loss function that captures the semantics of the problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Up until now\n",
    "- We have been  using pre-defined Loss functions (e.g., `binary_crossentropy`)\n",
    "- Specifying the Loss function in the compile statement\n",
    ">`model.compile(loss='binary_crossentropy')`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "You can [write your own loss functions](https://keras.io/api/losses/)\n",
    "\n",
    "In Keras, a Loss function has the signature\n",
    ">`loss_fn(y_true, y_pred, sample_weight=None)`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Custom train step (override `train_step`)\n",
    "\n",
    "But what if your Loss function needs access to values that are not part of the signature ?\n",
    "\n",
    "Or what if you want to change the training loop ?\n",
    "\n",
    "You could write your own training loop by overriding the `fit` method\n",
    "- Cycle through epochs\n",
    "- Within each epoch, cycle through mini-batches of examples\n",
    "- For each mini-batch of examples: execute the *train step*\n",
    "    - forward pass: feed input examples to Input layer, obtain output\n",
    "    - compute the loss\n",
    "    - Compute the gradient of the loss with respect to the weights\n",
    "    - Update the weights\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Rather than overriding `fit`, it sometimes suffices to override the train step: `train_step`\n",
    "\n",
    "Let's start by looking at the \"standard\" implementation of a basic train step.\n",
    "\n",
    "We will see\n",
    "- How losses are computed\n",
    "- Gradients are obtained\n",
    "- Weights are updated\n",
    "\n",
    "[Basic `train_step`](https://colab.research.google.com/github/tensorflow/docs/blob/snapshot-keras/site/en/guide/keras/customizing_what_happens_in_fit.ipynb#scrollTo=9022333acaa7&line=1&uniqifier=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We can modify the basic training step too.\n",
    "\n",
    "For example: suppose we want to make some training examples \"more important\" than others\n",
    "- Rather than Total Loss as equally-weighted average over all examples\n",
    "- Pass in per-example weights\n",
    "\n",
    "This might be useful, for example, when dealing with Imbalanced Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# `Layer` specialization\n",
    "\n",
    "A `Layer` in Keras is an abstract (Python) object\n",
    "- instantiating the object returns a function\n",
    "    - That maps input to the layer to the output\n",
    "\n",
    "We have used specific instances of `Layer` objects (e.g., `Dense`) as arguments in the list passed to the `Sequential` model type.\n",
    "\n",
    "We can also use instances in the Functional Model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "For example\n",
    "- `Dense(10)`\n",
    "    - Is the constructor for a fully connected layer instance with 10 units\n",
    "    - The constructor returns a function\n",
    "    - The the function maps the layer inputs to the outputs of the computation defined by the layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "So you will see code fragments like\n",
    ">\n",
    "    x = Input(shape=(784))\n",
    "    x = Dense(10, activation=softmax)(x)\n",
    "    \n",
    "- Re-using the variable `x` as the output of the current layer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "When the function is invoked, the Layer's `call` method is used\n",
    "- `call` gets invoked implicitly by \"parenthesized argument\" juxtaposition\n",
    "    - e.g., `Dense(10) ( x )`\n",
    "    - is similar to `obj= `Dense(10); result = obj.call(x)`\n",
    "- The function maps the inputs to the layer to the output\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Overriding `call` allows us to defined a new `Layer` sub-class.\n",
    "\n",
    "For example, [here](https://colab.research.google.com/github/keras-team/keras-io/blob/master/examples/nlp/ipynb/neural_machine_translation_with_transformer.ipynb#scrollTo=AupqfNAYaCHn&line=2&uniqifier=1) is the code defining some new `Layer` types that will be used to create a `Transformer` layer type."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The output of `Dense(10)` is a Tensor with final dimension equal to the number of units (e.g., 10)\n",
    "- The Tensor has leading dimensions too\n",
    "    - e.g., the implicit \"batch index\" dimension\n",
    "    - since the layer takes a mini-batch of examples (rather than a single example) as input\n",
    "- It may have *additional* dimensions too !\n",
    "    - Just like `numpy`: threading over additional dimensions\n",
    "    - e.g., if input is shape $(\\text{minibatch_size} \\times n_1 \\times n_2)$\n",
    "        - output is shape $(\\text{minibatch_size} \\times n_1 \\times 10)$\n",
    "        - `Dense` operates over the final dimension"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Studying advanced models\n",
    "\n",
    "The best way to learn is to study the code of some non-trivial models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Transformer: Custom layers, Skip connections, Layer Norm\n",
    "\n",
    "We have already seen part of the Transformer in introducing the basics of the Functional model.\n",
    "\n",
    "We use the rest of this example to discover other advanced Keras techniques:\n",
    "\n",
    "[Transformer layer](https://colab.research.google.com/github/keras-team/keras-io/blob/master/examples/nlp/ipynb/neural_machine_translation_with_transformer.ipynb#scrollTo=4DaEQr-lMkSs)\n",
    "- [Custom layers](https://colab.research.google.com/github/keras-team/keras-io/blob/master/examples/nlp/ipynb/neural_machine_translation_with_transformer.ipynb#scrollTo=Ywxggf9Anabk&line=8&uniqifier=1)\n",
    "- Layer Norm\n",
    "- Skip connections"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Custom layers: subtle point\n",
    "\n",
    "Let's look at the constructor for the `TransformerEncoder` custom layer, as an example\n",
    "\n",
    "    class TransformerEncoder(layers.Layer):\n",
    "        def __init__(self, embed_dim, dense_dim, num_heads, **kwargs):\n",
    "            super(TransformerEncoder, self).__init__(**kwargs)\n",
    "            self.embed_dim = embed_dim\n",
    "            self.dense_dim = dense_dim\n",
    "            self.num_heads = num_heads\n",
    "            self.attention = layers.MultiHeadAttention(\n",
    "                num_heads=num_heads, key_dim=embed_dim\n",
    "            )\n",
    "            self.dense_proj = keras.Sequential(\n",
    "                [layers.Dense(dense_dim, activation=\"relu\"), layers.Dense(embed_dim),]\n",
    "            )\n",
    "            self.layernorm_1 = layers.LayerNormalization()\n",
    "            self.layernorm_2 = layers.LayerNormalization()\n",
    "            self.supports_masking = True\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The custom layer consists of a collection of component layers\n",
    "\n",
    "Why are the components layers (e.g., Dense, MultiHeadAttention, LayerNormalization) instantiated in the class constructor\n",
    "- As opposed to being defined in the `call` method \n",
    "\n",
    "Had we instantiated each component within the `call` method\n",
    "- There would be a *new instance* of each component *each time the layer was called on an example* in training !\n",
    "- Each instance would have *it's own weights*\n",
    "- So training would not \"learn\" between examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Other custom layers of interest\n",
    "\n",
    "We can dig deeper to examine how the Attention layers are implemented in code:\n",
    "- [Scaled dot-product attention](https://www.tensorflow.org/text/tutorials/transformer#scaled_dot_product_attention)\n",
    "- [Multi-head attention](https://www.tensorflow.org/text/tutorials/transformer#multi-head_attention)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## VAE: Custom Model -- Training Loop, the Gradient Tape\n",
    "\n",
    "\n",
    "\n",
    "[Variational Autoencoder (VAE) from github](https://colab.research.google.com/github/keras-team/keras-io/blob/master/examples/generative/ipynb/vae.ipynb#scrollTo=DEU05Oe0vJrY)\n",
    "- [VAE: Custom train step](https://colab.research.google.com/github/keras-team/keras-io/blob/master/examples/generative/ipynb/vae.ipynb#scrollTo=0EHkZ1WCHw9E)\n",
    "    - Complex loss\n",
    "    \n",
    "We use this example to illustrate\n",
    "- How to sub-class the `Model` abstract class\n",
    "- How to create a custom training loop\n",
    "    - the \"Gradient tape\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Issues**\n",
    "- Custom **model** (not layer) class VAE\n",
    "- The *reconstruction loss* depends on the output of the Decoder part of the VAE\n",
    "    - No other obvious way to define this loss aside from a **custom training** step\n",
    "- Because we are computing the Loss in the training step\n",
    "    - we must compute the gradient of the Loss w.r.t weights\n",
    "    - Update the weights (**gradient tape**)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Visualizing what CNN's learn: Gradient Ascent and the Gradient Tape\n",
    "\n",
    "[Visualizing what Convnets learn](https://colab.research.google.com/github/keras-team/keras-io/blob/master/examples/vision/ipynb/visualizing_what_convnets_learn.ipynb#scrollTo=K8ITQAj7FTZd)\n",
    "- The Gradient Tape\n",
    "- Maximize utility (negative loss)\n",
    "    - mean (across the spatial dimensions) of one feature map in a multi-layer CNN\n",
    "    - the \"weights\" being solved for are the pixels of the input image !\n",
    "\n",
    "We use this example to show how powerful the Gradient Tape is\n",
    "\n",
    "[Gradient ascent](https://colab.research.google.com/github/keras-team/keras-io/blob/master/examples/vision/ipynb/visualizing_what_convnets_learn.ipynb#scrollTo=a9hZnslRFTZZ)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Factor Models and Autoencoders: Threading\n",
    "\n",
    "We use this example to show\n",
    "- A Functional model applied to a common problem in Finance.\n",
    "- Threading\n",
    "\n",
    "We will cover the Finance aspects of this in a [separate module](Autoencoder_for_conditional_risk_factors.ipynb)\n",
    "\n",
    "For now, I want to focus on the idea and the code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Here is the code, excerpted from the [notebook](https://github.com/stefan-jansen/machine-learning-for-trading/blob/main/20_autoencoders_for_conditional_risk_factors/06_conditional_autoencoder_for_asset_pricing_model.ipynb)\n",
    "\n",
    "    def make_model(hidden_units=8, n_factors=3):\n",
    "        input_beta = Input((n_tickers, n_characteristics), name='input_beta')\n",
    "        input_factor = Input((n_tickers,), name='input_factor')\n",
    "\n",
    "        hidden_layer = Dense(units=hidden_units, activation='relu', name='hidden_layer')(input_beta)\n",
    "        batch_norm = BatchNormalization(name='batch_norm')(hidden_layer)\n",
    "\n",
    "        output_beta = Dense(units=n_factors, name='output_beta')(batch_norm)\n",
    "\n",
    "        output_factor = Dense(units=n_factors, name='output_factor')(input_factor)\n",
    "\n",
    "        output = Dot(axes=(2,1), name='output_layer')([output_beta, output_factor])\n",
    "\n",
    "        model = Model(inputs=[input_beta, input_factor], outputs=output)\n",
    "        model.compile(loss='mse', optimizer='adam')\n",
    "        return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Not obvious what is going on here.\n",
    "\n",
    "A picture will help:\n",
    "\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <th><center>Autoencoder for Conditional Risk Factors</center></th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td><img src=\"images/Autoencoder_for_conditional_risk_factors.png\" width=\"90%\"></td>\n",
    "    </tr>\n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "$$\n",
    "\\newcommand{\\R}{\\mathbf{R}}\n",
    "\\newcommand{\\r}{\\mathbf{r}}\n",
    "\\newcommand{\\F}{\\mathbf{F}}\n",
    "\\newcommand{\\V}{\\mathbf{V}}\n",
    "\\newcommand{\\ntickers}{{n_\\text{tickers}}}\n",
    "\\newcommand{\\ndates}{{n_\\text{dates}}}\n",
    "\\newcommand{\\nfactors}{{n_\\text{factors}}}\n",
    "\\newcommand{\\nchars}{{n_\\text{chars}}}\n",
    "\\newcommand{\\dp}{{(d)}}\n",
    "\\newcommand{\\sp}{{(s)}}\n",
    "\\newcommand{\\Bbeta}{\\mathbf\\beta}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Threading \n",
    "\n",
    "Let's focus on the `Dense` layer corresponding to the box labelled \"Beta\" in the picture\n",
    "\n",
    "- `Dense` $( \\nfactors )$\n",
    "\n",
    "From the diagram you will notice that \n",
    "- the input to this  layer is *two dimensional*: $(\\ntickers \\times \\nchars)$\n",
    "- the output to this is *two dimensional*: $(\\ntickers \\times \\nfactors)$\n",
    "\n",
    "We have not yet seen multi-dimensional input/output in regard to a `Dense` layer\n",
    "\n",
    "What is going on here ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The layer is implementing a function with signature\n",
    "- `Dense`( $\\nfactors ) :  (\\ntickers \\times \\nchars) \\mapsto (\\ntickers \\times \\nfactors) $\n",
    "\n",
    "Tensorflow/Keras works on higher dimensional objects just like NumPy: \n",
    "- [threading](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Dense) over \"extra\" dimensions\n",
    "\n",
    "If the input to layer $\\ll$ is shape $(\\dim_{\\llp,1} \\times \\dim_{\\llp,2} \\times \\ldots \\dim_{\\llp,N} \\,\\, \\times n_\\llp )$\n",
    "- And the layer type operates over a *single* dimension (usually the last dimension)\n",
    "    - producing output shape $n_{(\\ll+1)}$    \n",
    "\n",
    "Then threading treats the inputs\n",
    "- as a tensor of shape $(\\dim_{\\llp,1} \\times \\dim_{\\llp,2} \\times \\ldots \\dim_{\\llp,N} )$ instances, each of shape $n_\\llp $\n",
    "\n",
    "Producing an output of shape \n",
    "- If the input to layer $\\ll$ is shape $(\\dim_{\\llp,1} \\times \\dim_{\\llp,2} \\times \\ldots \\dim_{\\llp,N} \\,\\, \\times n_{(\\ll+1)} )$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "In our case\n",
    "- Input shape is $(\\ntickers \\times \\nchars)$\n",
    "- The `Dense` layer is defined with $\\nfactors$ units ($n_{(\\ll+1)} = \\nfactors$)\n",
    "- Hence, the output shape is $(\\ntickers \\times \\nfactors)$\n",
    "\n",
    "The weight matrix for this layer\n",
    "- $\\W_\\beta$ with shape $( \\nfactors \\times \\nchars )$\n",
    "    - just like any `Dense` layer; number of weights is *independent* of threading\n",
    "- applies the *same weights* to each of the $\\ntickers$ (the rows) of the input\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Neural Style Transfer: Feature extractor, Training Loop\n",
    "\n",
    "[Neural Style Transfer](https://keras.io/examples/generative/neural_style_transfer/)\n",
    "- [Complex Loss](](https://keras.io/examples/generative/neural_style_transfer/#compute-the-style-transfer-loss))\n",
    "- Custom training loop\n",
    "- [Feature extractor](https://keras.io/examples/generative/neural_style_transfer/#compute-the-style-transfer-loss)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do you remember the Neural Style Transfer task\n",
    "- that we used to\n",
    "preview the concept that Deep Learning is all about defining a Loss Function\n",
    "that captures the semantics of the task ?\n",
    "\n",
    "That is, the task is\n",
    "- Given\n",
    "    - a Style image <img src=images/starry_night_crop.jpg width=10%>\n",
    "    - and a Content image <img src=images/chicago.jpg width=10%>\n",
    "- Generate an image that is the Content image re-drawn in the \"style\" of the Style image\n",
    "<img src=images/chicago_starry_night.jpg width=10%>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We use this example to illustrate\n",
    "- Complex Loss and Custom Training Loop\n",
    "- [Feature extractor](https://keras.io/examples/generative/neural_style_transfer/#compute-the-style-transfer-loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Content Loss and Style Loss\n",
    "\n",
    "The objective of Neural Style Transfer:\n",
    "- Given Content Image $C$\n",
    "- Give Style Image $S$\n",
    "- Create Generated Image $G$\n",
    "- Minimizing\n",
    "$$\n",
    "\\loss = \\loss_\\text{content} + \\loss_\\text{style}\n",
    "$$\n",
    "- where\n",
    "    - $\\loss_\\text{content}$ measures the dissimilarity of the \"content\" of $G$ and \"content\" f $C$\n",
    "    - $\\loss_\\text{style}$ measures the dissimilarity of  the \"style\" of $G$ and \"style\" of $C$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "How do we measure the dissimilarity of the \"content\" ?\n",
    "\n",
    "We can't just use plain MSE of the pixel-wise differences\n",
    "- $G$ is different than $C$, by definition (the \"styles\" are different)\n",
    "\n",
    "And how do we define what the \"style\" of an image is ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "$$\n",
    "\\newcommand{\\ICL}{\\mathbb{C}}\n",
    "\\newcommand{\\GM}{\\mathbb{G}}$\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Suppose we have an Image Classifier Neural Network $\\ICL$ (e.g., VGG19).\n",
    "\n",
    "Further suppose $\\ICL$ consists of a sequence of CNN Layers\n",
    "- Let $\\ICL_\\llp$ denote the set of $n_\\llp$ feature maps produced at layer $\\ll$\n",
    "    - Feature map: value of one feature, at each spatial location"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We choose \n",
    "- one layer $\\ll_c$ of $\\ICL$ and call it the \"content representation\" layer\n",
    "    - Will tend to be shallow: closer to the input\n",
    "    - Features of shallow layers will be more \"syntax\" than \"semantics\"\n",
    "- one layer $\\ll_s$ of $\\ICL$ and call it the \"style representation\" layer\n",
    "  - Will tend to be deep: closer to the output\n",
    "    - Features of deep layers will be more  \"semantics\" than \"syntax\"\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "For arbitrary image $I$, let\n",
    "- $\\ICL_{(\\ll_c)}(I)$ \n",
    "    - denote the feature maps of the Classifier $\\ICL$, on image $I$,  at the \"content representation\" layer\n",
    "- $\\ICL_{(\\ll_s)}(I)$\n",
    "    - denote the feature maps of the Classifier $\\ICL$, on image $I$, at the \"style representation\" layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We can now define the dissimilarity of the \"content\" of Content Image $C$ and \"content\" f Generated Image $G$\n",
    "- by comparing $\\ICL_{(\\ll_c)}(C)$ and $\\ICL_{(\\ll_c)}(G)$\n",
    "\n",
    "Similarly, we can define the dissimilarity of the \"style\" of Content Image $C$ and \"style\" of Generated Image $G$\n",
    "- by comparing $\\ICL_{(\\ll_s)}(S)$ and $\\ICL_{(\\ll_s)}(G)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "For any image $I$: $\\ICL_{(\\ll)}(I)$ consists of $n_\\llp$ layers.\n",
    "\n",
    "We need to define what it means to compare  $\\ICL_{(\\ll)}(I)$ and  $\\ICL_{(\\ll)}(I')$.\n",
    "\n",
    "The *Gramm Matrix $\\GM$* of $\\ICL_{(\\ll)}(I)$ \n",
    "- Has shape ($n_\\llp \\times n_\\llp$)\n",
    "- $\\GM_{j,j'}(I) = \\text{correlation}( \\text{flatten}(\\ICL_{(\\ll),j}(I)), \\text{flatten}(\\ICL_{(\\ll),j'}(I)) )$\n",
    "    - the correlation of the feature map $j$ of $\\ICL_{(\\ll)}(I)$ with feature map $j'$ of $\\ICL_{(\\ll)}(I')$\n",
    "    \n",
    "Intuitively, the Gramm Matrix measures the correlation of the values across pixel locations (flattened feature maps)\n",
    "of two feature maps of image $I$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We can now define the dissimilarity of $\\ICL_{(\\ll)}(I)$ and  $\\ICL_{(\\ll)}(I')$\n",
    "- As the MSE\n",
    "- of $\\GM(I)$ and $\\GM(I')$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Using this dissimilarity measure, we can define the\n",
    "- $\\loss_\\text{content}$ as the dissimilarity of $\\ICL_{(\\ll_c)}(C)$ and  $\\ICL_{(\\ll_c)}(G)$\n",
    "- $\\loss_\\text{style}$ as the dissimilarity of $\\ICL_{(\\ll_s)}(S)$ and  $\\ICL_{(\\ll_c)}(G)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Gradient ascent: generating $G$\n",
    "\n",
    "We can find image $G$ via Gradient Ascent\n",
    "- Initialize $G$ to noise\n",
    "- Update pixel $G_{i, i', k}$ by $- \\frac{\\partial \\loss}{G_{i, i', k}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Feature extractor\n",
    "\n",
    "One key coding trick that we will illustrate\n",
    "- Obtaining the feature maps of the Classifier $\\ICL$, on image $I$,  at an arbitrary layer\n",
    "\n",
    "We will call this tool the *feature extractor*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "[Here](https://www.tensorflow.org/tutorials/generative/style_transfer) is a tutorial view of the notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Autoencoder: Functional model\n",
    "\n",
    "\n",
    "<!--- #include (Autoencoder_example.ipynb)) --->\n",
    " [Autoencoder example from github](https://colab.research.google.com/github/kenperry-public/ML_Spring_2022/blob/master/Autoencoder_example.ipynb)\n",
    "- Functional model\n",
    "\n",
    "**Issues**\n",
    "- We could use a Sequential model with initial Encoder layers and final Decoder layers\n",
    "    - But we would not be able to independently access the Encoder nor the Decoder as isolated models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## GAN\n",
    "[Simple GAN](https://keras.io/examples/generative/dcgan_overriding_train_step)\n",
    "- [Custom train step: GAN training](https://keras.io/examples/generative/dcgan_overriding_train_step/#override-trainstep)             "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Wasserstein GAN with Gradient Penalty\n",
    "[Wasserstein GAN with Gradient Penalty](https://keras.io/examples/generative/wgan_gp/#create-the-wgangp-model)\n",
    "- [Gradient Tape: used for loss term, rather than weight update](https://keras.io/examples/generative/wgan_gp/#create-the-wgangp-model)\n",
    "- [Overide `compile`](https://keras.io/examples/generative/wgan_gp/#create-the-wgangp-model)\n",
    "- [Custom train step: GAN training](https://keras.io/examples/generative/wgan_gp/#create-the-wgangp-model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "print(\"Done\")"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "369.594px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
